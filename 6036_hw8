#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Oct 27 22:08:48 2020

@author: gliu
"""

import math

class Sequential:
    def __init__(self, modules, loss):            
        self.modules = modules
        self.loss = loss

    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):
        D, N = X.shape

        np.random.seed(0)
        num_updates = 0
        indices = np.arange(N)
        while num_updates < iters:

            np.random.shuffle(indices)
            X = None  # Your code
            Y = None  # Your code

            for j in range(math.floor(N/K)):
                if num_updates >= iters: break

                # Implement the main part of mini_gd here
                Xt = None # Your code
                Yt = None # Your code

                # The rest of this function should be similar to your
                # implementation of Sequential.sgd in HW 7
                # Your code
                
                num_updates += 1

    def forward(self, Xt):                        
        for m in self.modules: Xt = m.forward(Xt)
        return Xt

    def backward(self, delta):                   
        for m in self.modules[::-1]: delta = m.backward(delta)

    def sgd_step(self, lrate):    
        for m in self.modules: m.sgd_step(lrate)




# My Answer


import math
class Sequential:
    def __init__(self, modules, loss):            
        self.modules = modules
        self.loss = loss

    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):
        D, N = X.shape

        np.random.seed(0)
        num_updates = 0
        indices = np.arange(N)
        while num_updates < iters:

            np.random.shuffle(indices)

            print(indices)
            print(X.shape)
            print(Y.shape)

            X = X[:,indices].copy()  # Shuffle the indices
            Y = Y[:,indices].copy()  # Shuffle the indices

            print(X.shape)
            print(Y.shape)

            for j in range(math.floor(N/K)):
                if num_updates >= iters: break
                # Implement the main part of mini_gd here
                # Your code
                Xt = X[:,K*j:K*(j+1)]  # Your code
                Yt = Y[:,K*j:K*(j+1)]  # Your code

                print(j)
                print(Xt.shape)
                print(Yt.shape)

                # The rest of this function should be similar to your
                # implementation of Sequential.sgd in HW 7
                # Your code
                
                self.X =  Xt
                self.Y =  Yt 
                # Forward Pass, Compute Ypred and Xt
                Ypred = self.forward(self.X)
            
                # Compute Loss
                delta = self.loss.forward(Ypred,self.Y)
            
                # Compute dLdZ
                dLdZ  = self.loss.backward() 
            
                # Backward Pass
                self.backward(dLdZ)
                
                # Update weights
                self.sgd_step(lrate)


                
                num_updates += 1
        return self
    def forward(self, Xt):                        
        for m in self.modules: Xt = m.forward(Xt)
        return Xt

    def backward(self, delta):                   
        for m in self.modules[::-1]: delta = m.backward(delta)

    def sgd_step(self, lrate):    
        for m in self.modules: m.sgd_step(lrate)
        
        
# Staff Answer

import math

class Sequential:
    def __init__(self, modules, loss):            
        self.modules = modules
        self.loss = loss

    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):
        D, N = X.shape

        np.random.seed(0)
        num_updates = 0
        indices = np.arange(N)
        while num_updates < iters:

            np.random.shuffle(indices)
            X = X[:,indices]
            Y = Y[:,indices]

            for j in range(math.floor(N/K)):
                if num_updates >= iters: break

                Xt = X[:, j*K:(j+1)*K]
                Yt = Y[:, j*K:(j+1)*K]

                Ypred = self.forward(Xt)
                loss = self.loss.forward(Ypred, Yt)

                err = self.loss.backward()
                self.backward(err)
                self.sgd_step(lrate)     
                num_updates += 1

    def forward(self, Xt):                        
        for m in self.modules: Xt = m.forward(Xt)
        return Xt

    def backward(self, delta):                   
        for m in self.modules[::-1]: delta = m.backward(delta)

    def sgd_step(self, lrate):    
        for m in self.modules: m.sgd_step(lrate)
        
        
        
#%% 1B Staff Answer
class BatchNorm(Module):    
    def __init__(self, m):
        np.random.seed(0)
        self.eps = 1e-20
        self.m = m  # number of input channels
        
        # Init learned shifts and scaling factors
        self.B = np.zeros([self.m, 1]) # m x 1
        self.G = np.random.normal(0, 1.0 * self.m ** (-.5), [self.m, 1]) # m x 1
        
    def forward(self, Z):
        # Z is n^l x K: m input channels and mini-batch size K
        # Store last inputs and K for next backward() call
        self.Z = Z
        self.K = Z.shape[1]
        
        self.mus = np.mean(self.Z, axis=1, keepdims=True)
        self.vars = np.var(self.Z, axis=1, keepdims=True)

        # Normalize inputs using their mean and standard deviation
        self.norm = (self.Z - self.mus) / (np.sqrt(self.vars) + self.eps)
            
        # Return scaled and shifted versions of self.norm
        return (self.G * self.norm) + self.B

    def backward(self, dLdZ):
        # Re-usable constants
        std_inv = 1/np.sqrt(self.vars+self.eps)
        Z_min_mu = self.Z-self.mus
        
        dLdnorm = dLdZ * self.G
        dLdVar = np.sum(dLdnorm * Z_min_mu * -0.5 * std_inv**3, axis=1, keepdims=True)
        dLdMu = np.sum(dLdnorm*(-std_inv), axis=1, keepdims=True) + dLdVar * (-2/self.K) * np.sum(Z_min_mu, axis=1, keepdims=True)
        dLdX = (dLdnorm * std_inv) + (dLdVar * (2/self.K) * Z_min_mu) + (dLdMu/self.K)
        
        self.dLdB = np.sum(dLdZ, axis=1, keepdims=True)
        self.dLdG = np.sum(dLdZ * self.norm, axis=1, keepdims=True)
        return dLdX

    def sgd_step(self, lrate):
        self.B -= lrate*self.dLdB
        self.G -= lrate*self.dLdG
        return
        
#%% My Answer
class BatchNorm(Module):    
    def __init__(self, m):
        np.random.seed(0)
        self.eps = 1e-20
        self.m = m  # number of input channels
        
        # Init learned shifts and scaling factors
        self.B = np.zeros([self.m, 1]) # m x 1
        self.G = np.random.normal(0, 1.0 * self.m ** (-.5), [self.m, 1]) # m x 1
        
    # Works on m x b matrices of m input channels and b different inputs
    def forward(self, Z):# Z is m x K: m input channels and mini-batch size K
        # Store last inputs and K for next backward() call
        self.Z = Z
        self.K = Z.shape[1]
        
        self.mus  = np.mean(Z,axis=1,keepdims=True) 
        self.vars = np.var(Z,axis=1,keepdims=True)  # Your Code

        # Normalize inputs using their mean and standard deviation
        self.norm = (self.Z-self.mus)/(np.sqrt(self.vars)+1e-16)  # Your Code
            
        # Return scaled and shifted versions of self.norm
        return self.G*self.norm+self.B  # Your Code

    def backward(self, dLdZ):
        # Re-usable constants
        std_inv = 1/np.sqrt(self.vars+self.eps)
        A_min_mu = self.Z-self.mus
        
        dLdnorm = dLdZ * self.G
        dLdVar = np.sum(dLdnorm * A_min_mu * -0.5 * std_inv**3, axis=1, keepdims=True)
        dLdMu = np.sum(dLdnorm*(-std_inv), axis=1, keepdims=True) + dLdVar * (-2/self.K) * np.sum(A_min_mu, axis=1, keepdims=True)
        dLdX = (dLdnorm * std_inv) + (dLdVar * (2/self.K) * A_min_mu) + (dLdMu/self.K)
        
        self.dLdB = np.sum(dLdZ, axis=1, keepdims=True)
        self.dLdG = np.sum(dLdZ * self.norm, axis=1, keepdims=True)
        return dLdX

    def sgd_step(self, lrate):
        self.B = self.B - lrate*self.dLdB  # Your Code
        self.G = self.G - lrate*self.dLdG  # Your Code
        return
